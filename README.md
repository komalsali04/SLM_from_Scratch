# TinyGPT - A Small Language Model for Story Generation

This project is a from-scratch implementation of a small, GPT-style language model in PyTorch. The model is trained on the **TinyStories dataset**, a collection of short stories generated by GPT-3.5 and GPT-4 that use a vocabulary understandable by young children. The goal is to build a compact and coherent text generation model with approximately **30 million parameters**.

The entire process is contained within a single Python script **(slm_for_tinystories.py)**, which handles everything from dataset downloading and tokenization to training and inference.

## Features
- Built from Scratch: The model architecture is implemented from the ground up using core PyTorch modules, providing a clear view of the inner workings of a transformer.

- GPT-2 Architecture: Implements a standard decoder-only transformer architecture, including multi-head causal self-attention, MLPs, and layer normalization.

- Efficient Training: Incorporates modern training optimizations:

- Mixed-Precision Training: Uses torch.amp.autocast with bfloat16 or float16 to speed up training and reduce memory usage.

- Flash Attention: Automatically uses the highly optimized F.scaled_dot_product_attention if available.

- AdamW Optimizer: Employs the AdamW optimizer with weight decay for better regularization.

- Learning Rate Scheduling: Uses a scheduler with linear warmup followed by cosine decay.

- Gradient Accumulation & Clipping: Processes larger effective batch sizes and ensures training stability.

- Memory-Mapped Data: The tokenized dataset is stored on disk using numpy.memmap, allowing for efficient data loading without consuming excessive RAM.

  ## Model Architecture

The model is a decoder-only transformer defined by the GPT class. Its configuration is managed by the GPTConfig dataclass.

The specific configuration used in this project is:

- Vocabulary Size (vocab_size): 50,257 (from GPT-2 tokenizer)

- Context Window (block_size): 128 tokens

- Transformer Blocks (n_layer): 6

- Attention Heads (n_head): 6

- Embedding Dimension (n_embd): 384

- Dropout Rate (dropout): 0.1

This configuration results in a model with approximately 30 million parameters, making it a "Small Language Model" (SLM).

